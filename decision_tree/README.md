# Decision Tree with the ID3 Algorithm

Decision tree [1] is a non-parametric supervised learning method based on a structure composed of nodes where decisions are made based on the value of a given feature. The choice is made at the leaf nodes based on the decisions made at each node.

To build (train) a decision tree model, algorithms such as ID3 (Iterative Dichotomiser 3) [2], C4.5, C5.0, and CART (Classification and Regression Tree) can be used. Here, the ID3 algorithm will be implemented and explored, which utilizes the concepts of entropy and information gain (mutual information). Through it, it is possible to create models for binary classification problems, where the target has only two possible values: 0 or 1. In the end, the developed algorithm will be applied to a bank churn prediction problem, that is, to predict whether a customer will leave or not leave a given banking service.

## Data

Binary Classification Bank Churn Dataset Cleaned from Kaggle [4]

<p align="center">
    <img width="800" src="https://github.com/Samirnunes/ml-algorithms-from-scratch/blob/main/decision_tree/images/data_image.png" alt="Material Bread logo">
<p>

## Implementation

The implementation of the decision tree and the ID3 algorithm was done in Python, with the help of the Pandas and Numpy libraries. Basically, it was done in three steps: defining a tree node, creating the ID3 algorithm, and creating the decision tree structure. Initially, the data structure Node, which represents a tree node, was defined. It is responsible for storing information such as the dataset seen by the node, the two child nodes (called `true_child` and `false_child`, with true being the node generated by the decision when the feature condition meets the greater or equal sign, and false being the opposite), the target, the node's prediction, its entropy, and information about the node's split into its children (feature used for the split, feature value used for the split condition, and information gain from the split). This is the data structure that will function as the fundamental component of the decision tree. The entropy of a discrete random variable X is given by the following equation [3]:

<p align="center">
    <img width="400" src="https://github.com/Samirnunes/ml-algorithms-from-scratch/blob/main/decision_tree/images/entropy.png" alt="Material Bread logo">
<p>

In the ID3 algorithm, we have, analogously, the concept of entropy and mutual information, which is called information gain in this context. Each node has a given distribution of the target variable, summarized as the number of zeros and ones of this variable that the node has due to the decisions already made, and its entropy relative to the target variable, here represented as the random variable Y, with y taking the values of 0 or 1. After a split into child nodes based on a feature, here represented by the random variable X, and a given value of it, each of the new nodes will also have an associated entropy. The information gain I(Y,X) is then given by the following equation [2]:

<p align="center">
    <img width="600" src="https://github.com/Samirnunes/ml-algorithms-from-scratch/blob/main/decision_tree/images/info_gain.png" alt="Material Bread logo">
<p>

In this equation, T represents the set of subsets created by splitting the node's data into two child nodes, and p(t) is the proportion of the number of elements in t relative to the dataset before the split. Here, the information gain represents how much information the feature used for the split provided about the target variable, analogous to what occurs in the binary channel. The goal of the ID3 algorithm is, therefore, to greedily choose the features and their respective values such that the information gains are as high as possible at each split, minimizing the entropy of the leaf nodes.

With this in mind, the algorithm was implemented in the `ID3` class, with a recursive tree construction. It starts at the root node with the entire dataset and searches for the feature and its value to perform the split of the data into two nodes with the highest possible information gain. This is done by iterating over all features and all their possible values, performing the splits and calculating each information gain, and finally selecting the one that generates the highest gain. The split considers a condition in the form â‰¥ (greater than or equal to) the feature value and < (less than) that value, generating a `true_node` in the first case and a `false_node` in the second, and when a feature is used, it cannot be used again for another split. Then, the construction is carried out recursively for the child nodes, until the stopping conditions are met: the node has only one target value (only 0 or 1) or there are no more features available for split. When one of these is satisfied, a leaf node is reached, through which it is possible to make a prediction if the feature values given as input lead to it. The construction is done through the build_tree function.

Finally, the `DecisionTree` class was implemented, through which decision tree models built with the ID3 algorithm can be generated and used for predictions. The train function trains a tree model given the training data: X, the dataframe containing the features, and y, the dataframe containing the target labels. Basically, it calls `build_tree` to construct the tree starting from its root node.

With the tree built, it is possible to use the predict function to make predictions given the feature values. The prediction is done recursively, traversing the tree as the input feature values satisfy or not the split conditions of each node. Finally, when the leaf node is reached, the prediction given by it is returned.

A method was also created to print the generated tree. It shows each node with its respective parents and children, as well as its entropy, information gain from the split into its children, feature used for the split, and the feature value threshold used in the split.

## Results

- Part of the built tree in the bank churn problem. The preprocessing for this problem uses a StandardScaler. [5]

<p align="center">
    <img width="800" src="https://github.com/Samirnunes/ml-algorithms-from-scratch/blob/main/decision_tree/images/tree.png" alt="Material Bread logo">
<p>

- Metrics [6] in the training set in the bank churn problem.

<p align="center">
    <img width="600" src="https://github.com/Samirnunes/ml-algorithms-from-scratch/blob/main/decision_tree/images/metrics.png" alt="Material Bread logo">
<p>

## References

 [1] https://towardsdatascience.com/decision-trees-for-classification-id3-algorithm-explained-89df76e72df1
 
 [2] https://en.wikipedia.org/wiki/ID3_algorithm
 
 [3] https://sgfin.github.io/files/notes/Cover_and_Thomas_ch2_entropy.pdf
 
 [4] https://www.kaggle.com/datasets/prishasawhney/binary-classification-bank-churn-dataset-cleaned?select=train_cleaned.csv
 
 [5] https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
 
 [6] https://www.evidentlyai.com/classification-metrics/accuracy-precision-recall


